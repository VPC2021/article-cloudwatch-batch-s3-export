AWSTemplateFormatVersion: '2010-09-09'
Description: 'CloudFormation template for CloudWatch Logs to S3 export solution'

Parameters:
  NotificationEmail:
    Type: String
    Description: Email address to receive notifications
  
  ChunkDays:
    Type: Number
    Description: Number of days per export chunk
    MinValue: 1
    MaxValue: 30

  BucketName:
    Type: String
    Description: Name of the S3 bucket to store exported logs

  GlacierTransitionDays:
    Type: Number
    Description: Number of days after which objects transition to Glacier

  ExportTagKey:
    Type: String
    Description: Tag key to identify log groups for export
    
  ExportTagValue:
    Type: String
    Description: Tag value to identify log groups for export

  ScheduleExpression:
    Type: String
    Description: Schedule expression for the export task

Resources:
  ExportBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    UpdateReplacePolicy: Retain
    Properties:
      BucketName: !Ref BucketName
      LifecycleConfiguration:
        Rules:
          - Id: GlacierTransition
            Status: Enabled
            ExpirationInDays: !Ref ObjectDeletionDays
            NoncurrentVersionExpirationInDays: !Ref ObjectDeletionDays
            Transitions:
              - TransitionInDays: !Ref GlacierTransitionDays
                StorageClass: GLACIER
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256

  ExportBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref ExportBucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: AllowCloudWatchLogsExport
            Effect: Allow
            Principal:
              Service: logs.amazonaws.com
            Action:
              - s3:GetBucketAcl
            Resource: !GetAtt ExportBucket.Arn
            Condition:
              StringEquals:
                aws:SourceAccount: !Ref AWS::AccountId
              ArnLike:
                aws:SourceArn: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:*'
          - Sid: AllowCloudWatchLogsPutObject
            Effect: Allow
            Principal:
              Service: logs.amazonaws.com
            Action:
              - s3:PutObject
            Resource: !Sub '${ExportBucket.Arn}/*'
            Condition:
              StringEquals:
                s3:x-amz-acl: bucket-owner-full-control
                aws:SourceAccount: !Ref AWS::AccountId
              ArnLike:
                aws:SourceArn: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:*'

  LogExporterSNSTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub '${AWS::StackName}-notifications'
      Subscription:
        - Protocol: email
          Endpoint: !Ref NotificationEmail

  LogExporterRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: LogExporterPermissions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateExportTask
                  - logs:DescribeExportTasks
                  - logs:DescribeLogGroups
                  - logs:ListTagsLogGroup
                Resource: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*'
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource: !Sub '${ExportBucket.Arn}/*'
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: !Ref LogExporterSNSTopic
              - Effect: Allow
                Action:
                  - ssm:PutParameter
                  - ssm:GetParameter
                  - ssm:DeleteParameter
                Resource: '*'

  LogExporterFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${AWS::StackName}-exporter'
      Runtime: python3.9
      Handler: index.lambda_handler
      Timeout: 900
      MemorySize: 2048
      Role: !GetAtt LogExporterRole.Arn
      Environment:
        Variables:
          DESTINATION_BUCKET: !Ref ExportBucket
          SNS_TOPIC_ARN: !Ref LogExporterSNSTopic
          CHUNK_DAYS: !Ref ChunkDays
          MAX_CONCURRENT_EXPORTS: '1'
          ADAPTIVE_CHUNK_SIZE: 'true'
      Code:
        ZipFile: |
          import boto3
          import time
          from datetime import datetime, timezone, timedelta
          import logging
          import json
          from botocore.exceptions import ClientError
          from botocore.config import Config
          import os
          import random

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          
          # Initialize AWS clients with retry configuration
          retry_config = Config(
              retries = dict(
                  max_attempts = 10,
                  mode = 'adaptive'
              )
          )

          logs_client = boto3.client('logs', config=retry_config)
          ssm_client = boto3.client('ssm', config=retry_config)
          sns_client = boto3.client('sns', config=retry_config)

          def exponential_backoff(attempt, max_delay=32):
              """Calculate exponential backoff with jitter"""
              delay = min(max_delay, pow(2, attempt))
              jitter = random.uniform(0, 0.1 * delay)
              return delay + jitter

          class LogExporter:
              def __init__(self, destination_bucket, sns_topic_arn, export_tag_key, export_tag_value, chunk_days=1, lambda_context=None):
                  self.destination_bucket = destination_bucket
                  self.sns_topic_arn = sns_topic_arn
                  self.export_tag_key = export_tag_key
                  self.export_tag_value = export_tag_value
                  self.chunk_days = chunk_days
                  self.max_wait_time = 870  # 14.5 minutes to allow buffer for cleanup
                  self.max_retries = 10
                  self.current_task_start_time = None
                  self.lambda_context = lambda_context

              def format_bytes(self, size):
                  """Convert bytes to human readable format"""
                  for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
                      if size < 1024.0:
                          return f"{size:.2f} {unit}"
                      size /= 1024.0

              def calculate_time_remaining(self, progress_percentage, elapsed_time):
                  """Estimate remaining time based on current progress"""
                  if progress_percentage <= 0:
                      return "Calculating..."
                  
                  elapsed_seconds = time.time() - self.current_task_start_time
                  total_estimated_seconds = (elapsed_seconds * 100) / progress_percentage
                  remaining_seconds = total_estimated_seconds - elapsed_seconds
                  
                  if remaining_seconds < 60:
                      return f"{int(remaining_seconds)} seconds"
                  elif remaining_seconds < 3600:
                      return f"{int(remaining_seconds/60)} minutes"
                  else:
                      return f"{remaining_seconds/3600:.1f} hours"

              def exponential_backoff(self, attempt, max_delay=32):
                  """Calculate exponential backoff with jitter"""
                  delay = min(max_delay, pow(2, attempt))
                  jitter = random.uniform(0, 0.1 * delay)
                  return delay + jitter

              def retry_with_backoff(self, func, *args, **kwargs):
                  """Generic retry function with exponential backoff"""
                  for attempt in range(self.max_retries):
                      try:
                          return func(*args, **kwargs)
                      except ClientError as e:
                          if e.response['Error']['Code'] in ['ThrottlingException', 'TooManyRequestsException']:
                              if attempt == self.max_retries - 1:
                                  raise  # Re-raise the exception if we've exhausted all retries
                              
                              delay = self.exponential_backoff(attempt)
                              logger.warning(f"Request throttled, retrying in {delay:.2f} seconds (attempt {attempt + 1}/{self.max_retries})")
                              time.sleep(delay)
                          else:
                              raise

              def calculate_optimal_chunk_size(self, log_group_name):
                  """Calculate optimal chunk size based on log volume"""
                  try:
                      response = logs_client.describe_log_groups(
                          logGroupNamePrefix=log_group_name,
                          limit=1
                      )
                      stored_bytes = response['logGroups'][0].get('storedBytes', 0)
                      
                      logger.info(f"Log group {log_group_name} size: {stored_bytes/1_000_000_000:.2f} GB")
                      
                      # Adjust chunk days based on volume
                      if stored_bytes > 10_000_000_000:  # 10 GB
                          optimal_days = 3  # 3 days for large log groups
                      elif stored_bytes > 5_000_000_000:  # 5 GB
                          optimal_days = 5  # 5 days for medium log groups
                      else:
                          optimal_days = 7  # 7 days for small log groups
                      
                      logger.info(f"Using {optimal_days} days chunk size for {log_group_name}")
                      return optimal_days
                          
                  except Exception as e:
                      logger.warning(f"Error calculating chunk size: {str(e)}, using default {self.chunk_days} days")
                      return self.chunk_days  # Return default chunk size

              def send_sns_notification(self, subject, message):
                  """Send an SNS notification with enhanced formatting"""
                  try:
                      if not self.sns_topic_arn:
                          logger.warning("SNS topic ARN not provided, skipping notification")
                          return False
                        
                      # Add timestamp and environment info to the message
                      formatted_message = f"""
              {message}

              ---
              Timestamp: {datetime.now().isoformat()}
              Environment: {os.environ.get('AWS_EXECUTION_ENV', 'Unknown')}
              Region: {os.environ.get('AWS_REGION', 'Unknown')}
              Lambda Function: {os.environ.get('AWS_LAMBDA_FUNCTION_NAME', 'Unknown')}
              """
                    
                      # Send the notification
                      sns_client.publish(
                          TopicArn=self.sns_topic_arn,
                          Subject=subject[:100],  # SNS subject has a 100 character limit
                          Message=formatted_message
                      )
                    
                      logger.info(f"Sent SNS notification: {subject}")
                      return True
                  except Exception as e:
                      logger.error(f"Failed to send SNS notification: {str(e)}")
                      return False
              
              def get_log_group_tags(self, log_group_name):
                  """Get tags for a log group with retry logic"""
                  def get_tags():
                      return logs_client.list_tags_log_group(logGroupName=log_group_name)
                  
                  try:
                      response = self.retry_with_backoff(get_tags)
                      return response.get('tags', {})
                  except Exception as e:
                      logger.error(f"Error getting tags for log group {log_group_name}: {str(e)}")
                      return {}

              def get_tagged_log_groups(self):
                  """Get all log groups with specified tag (with retry logic)"""
                  tagged_groups = []
                  next_token = None
                  
                  while True:
                      try:
                          if next_token:
                              response = self.retry_with_backoff(
                                  logs_client.describe_log_groups,
                                  nextToken=next_token
                              )
                          else:
                              response = self.retry_with_backoff(
                                  logs_client.describe_log_groups
                              )
                          
                          for log_group in response.get('logGroups', []):
                              log_group_name = log_group['logGroupName']
                              tags = self.get_log_group_tags(log_group_name)
                              
                              if tags.get(self.export_tag_key) == self.export_tag_value:
                                  logger.info(f"Found tagged log group: {log_group_name}")
                                  tagged_groups.append(log_group_name)
                          
                          next_token = response.get('nextToken')
                          if not next_token:
                              break
                              
                      except Exception as e:
                          logger.error(f"Error listing log groups: {str(e)}")
                          break
                  
                  return tagged_groups

              def get_ssm_parameter_name(self, log_group_name, param_type='last-export'):
                  """Generate SSM parameter name for different tracking purposes"""
                  sanitized_name = log_group_name.lstrip('/').replace('/', '-')
                  return f"/log-exporter/{sanitized_name}/{param_type}"

              def store_export_progress(self, log_group_name, progress_data):
                  """Store export progress in SSM Parameter Store"""
                  try:
                      parameter_name = self.get_ssm_parameter_name(log_group_name, 'progress')
                      ssm_client.put_parameter(
                          Name=parameter_name,
                          Value=json.dumps(progress_data),
                          Type='String',
                          Overwrite=True
                      )
                      logger.info(f"Stored progress data for {log_group_name}")
                  except Exception as e:
                      logger.error(f"Error storing progress data: {str(e)}")

              def get_export_progress(self, log_group_name):
                  """Retrieve export progress from SSM Parameter Store"""
                  try:
                      parameter_name = self.get_ssm_parameter_name(log_group_name, 'progress')
                      response = ssm_client.get_parameter(Name=parameter_name)
                      return json.loads(response['Parameter']['Value'])
                  except ClientError as e:
                      if e.response['Error']['Code'] == 'ParameterNotFound':
                          return None
                      raise
                  except Exception as e:
                      logger.error(f"Error retrieving progress data: {str(e)}")
                      return None

              def update_export_status(self, log_group_name, task_id, status, current_chunk, total_chunks):
                  """Update export status with simplified progress tracking"""
                  try:
                      elapsed_time = time.time() - self.current_task_start_time
                      message = (
                          f"Export Progress for {log_group_name}:\n"
                          f"? Status: {status}\n"
                          f"? Progress: {(current_chunk/total_chunks)*100:.1f}%\n"
                          f"? Elapsed Time: {int(elapsed_time/60)} minutes {int(elapsed_time%60)} seconds"
                      )
                      logger.info(message)
                      
                      # Send notification for significant status changes
                      if status in ['COMPLETED', 'FAILED', 'CANCELLED']:
                          self.send_sns_notification(
                              f"Log Export {status} - {log_group_name}",
                              message
                          )
                  except Exception as e:
                      logger.error(f"Error updating export status: {str(e)}")


              def get_last_export_time(self, ssm_parameter_name):
                  """Get last export timestamp from SSM Parameter Store"""
                  try:
                      response = ssm_client.get_parameter(Name=ssm_parameter_name)
                      return int(response['Parameter']['Value'])
                  except ssm_client.exceptions.ParameterNotFound:
                      logger.info(f"SSM parameter {ssm_parameter_name} not found, creating with default value")
                      # Set a default timestamp (e.g., 24 hours ago)
                      default_timestamp = int((datetime.now() - timedelta(days=1)).timestamp() * 1000)
                      self.update_last_export_time(ssm_parameter_name, default_timestamp)
                      return default_timestamp
                  except Exception as e:
                      logger.error(f"Error retrieving SSM parameter: {str(e)}")
                      raise

              def check_and_acquire_lock(self, lock_parameter_name):
                  """Try to acquire a lock using SSM Parameter Store"""
                  try:
                      # Try to create a lock parameter with current timestamp
                      current_time = int(time.time())
                      expiry_time = current_time + self.max_wait_time  # Lock expires after max_wait_time

                      # Generate a unique ID if lambda_context is not available
                      request_id = self.lambda_context.aws_request_id if self.lambda_context else f"local-{uuid.uuid4()}"
                    
                      try:
                          # Try to create the parameter if it doesn't exist
                          ssm_client.put_parameter(
                              Name=lock_parameter_name,
                              Value=f"{expiry_time}|{self.lambda_context.aws_request_id}",
                              Type='String',
                              Overwrite=False  # Don't overwrite if it exists
                          )
                          logger.info(f"Acquired lock {lock_parameter_name}")
                          return True
                      except ssm_client.exceptions.ParameterAlreadyExists:
                          # Parameter exists, check if lock has expired
                          response = ssm_client.get_parameter(Name=lock_parameter_name)
                          lock_value = response['Parameter']['Value']
                          parts = lock_value.split('|')
                        
                          if len(parts) == 2:
                              lock_expiry, lock_owner = int(parts[0]), parts[1]
                             
                              if current_time > lock_expiry:
                                  # Lock has expired, take it over
                                  logger.info(f"Taking over expired lock {lock_parameter_name} from {lock_owner}")
                                  ssm_client.put_parameter(
                                      Name=lock_parameter_name,
                                      Value=f"{expiry_time}|{self.lambda_context.aws_request_id}",
                                      Type='String',
                                      Overwrite=True
                                  )
                                  return True
                              else:
                                  # Lock is still valid
                                  logger.info(f"Lock {lock_parameter_name} is held by {lock_owner}, cannot proceed")
                                  return False
                          else:
                              # Invalid lock format, overwrite it
                              logger.warning(f"Invalid lock format in {lock_parameter_name}, overwriting")
                              ssm_client.put_parameter(
                                  Name=lock_parameter_name,
                                  Value=f"{expiry_time}|{self.lambda_context.aws_request_id}",
                                  Type='String',
                                  Overwrite=True
                              )
                              return True
                  except Exception as e:
                      logger.error(f"Error acquiring lock: {str(e)}")
                      return False

              def release_lock(self, lock_parameter_name):
                  """Release the acquired lock"""
                  try:
                      response = ssm_client.get_parameter(Name=lock_parameter_name)
                      lock_value = response['Parameter']['Value']
                      parts = lock_value.split('|')

                      request_id = self.lambda_context.aws_request_id if self.lambda_context else None
                      
                      if len(parts) == 2 and (parts[1] == request_id or request_id is None):
                          # Only delete if we own the lock
                          ssm_client.delete_parameter(Name=lock_parameter_name)
                          logger.info(f"Released lock {lock_parameter_name}")
                          return True
                      else:
                          logger.warning(f"Cannot release lock {lock_parameter_name} as it's owned by another process")
                          return False
                  except Exception as e:
                      logger.error(f"Error releasing lock: {str(e)}")
                      return False

              def update_last_export_time(self, ssm_parameter_name, timestamp):
                  """Update last export timestamp in SSM Parameter Store"""
                  try:
                      ssm_client.put_parameter(
                          Name=ssm_parameter_name,
                          Value=str(timestamp),
                          Type='String',
                          Overwrite=True
                      )
                      logger.info(f"Updated SSM parameter {ssm_parameter_name} with timestamp {timestamp}")
                  except Exception as e:
                      logger.error(f"Failed to update SSM parameter: {str(e)}")
                      raise

              def record_task_execution(self, log_group_name, task_id, from_time, to_time, status):
                  """Record task execution details across multiple SSM Parameter Store values if needed"""
                  try:
                      # Base parameter name for this log group
                      base_param_name = f"/cloudwatch/export-history/{log_group_name.replace('/', '-')}"
                      
                      # New task entry with explicit type conversion to ensure proper JSON serialization
                      new_entry = {
                          'taskId': str(task_id),
                          'fromTime': int(from_time),
                          'toTime': int(to_time),
                          'fromTimeFormatted': datetime.fromtimestamp(from_time/1000).strftime('%Y-%m-%dT%H:%M:%S.%f'),
                          'toTimeFormatted': datetime.fromtimestamp(to_time/1000).strftime('%Y-%m-%dT%H:%M:%S.%f'),
                          'status': str(status),
                          'timestamp': int(time.time() * 1000)
                      }

                      # Validate new entry JSON serialization
                      try:
                          json.dumps(new_entry)
                          logger.debug(f"New entry validation successful: {json.dumps(new_entry, indent=2)}")
                      except json.JSONDecodeError as je:
                          logger.error(f"Invalid JSON format in new entry: {str(je)}")
                          logger.error(f"Problematic entry: {new_entry}")
                          return False

                      # Get the index parameter
                      index_param_name = f"{base_param_name}-index"
                      try:
                          response = ssm_client.get_parameter(Name=index_param_name)
                          index_data = json.loads(response['Parameter']['Value'])
                          current_index = int(index_data['currentIndex'])  # Ensure integer
                          total_params = int(index_data['totalParams'])    # Ensure integer
                      except ssm_client.exceptions.ParameterNotFound:
                          current_index = 0
                          total_params = 1
                          index_data = {'currentIndex': current_index, 'totalParams': total_params}
                      except json.JSONDecodeError as je:
                          logger.error(f"Invalid JSON in index parameter: {str(je)}")
                          current_index = 0
                          total_params = 1
                          index_data = {'currentIndex': current_index, 'totalParams': total_params}

                      # Get current parameter content
                      current_param_name = f"{base_param_name}-{current_index}"
                      try:
                          response = ssm_client.get_parameter(Name=current_param_name)
                          history = json.loads(response['Parameter']['Value'])
                          if not isinstance(history, list):
                              logger.error("History is not a list, resetting to empty list")
                              history = []
                      except ssm_client.exceptions.ParameterNotFound:
                          history = []
                      except json.JSONDecodeError as je:
                          logger.error(f"Invalid JSON in parameter {current_param_name}: {str(je)}")
                          history = []

                      # Add new entry and validate combined JSON
                      history.append(new_entry)
                      try:
                          history_json = json.dumps(history)
                          # Validate the combined JSON can be loaded back
                          json.loads(history_json)
                      except json.JSONDecodeError as je:
                          logger.error(f"Invalid JSON after adding new entry: {str(je)}")
                          return False

                      # Check size and handle parameter updates
                      if len(history_json.encode('utf-8')) > 3500:
                          logger.info(f"Parameter size limit reached ({len(history_json.encode('utf-8'))} bytes). Creating new parameter.")
                          current_index += 1
                          total_params = max(total_params, current_index + 1)
                          history = [new_entry]  # Start fresh with just the new entry
                          history_json = json.dumps(history)

                      # Update parameters with error handling
                      try:
                          ssm_client.put_parameter(
                              Name=current_param_name,
                              Value=history_json,
                              Type='String',
                              Overwrite=True
                          )
                      except Exception as e:
                          logger.error(f"Failed to update parameter {current_param_name}: {str(e)}")
                          return False

                      # Update index if needed
                      if len(history_json.encode('utf-8')) > 3500:
                          try:
                              index_json = json.dumps({
                                  'currentIndex': current_index,
                                  'totalParams': total_params
                              })
                              ssm_client.put_parameter(
                                  Name=index_param_name,
                                  Value=index_json,
                                  Type='String',
                                  Overwrite=True
                              )
                          except Exception as e:
                              logger.error(f"Failed to update index parameter: {str(e)}")
                              return False

                      return True

                  except Exception as e:
                      logger.error(f"Error recording task execution: {str(e)}")
                      logger.error(f"Task ID: {task_id}")
                      logger.error(f"From Time: {from_time}")
                      logger.error(f"To Time: {to_time}")
                      logger.error(f"Status: {status}")
                      return False



              def get_task_history(self, log_group_name):
                  """Get complete task history across multiple parameters"""
                  try:
                      # Base parameter name for this log group
                      base_param_name = f"/cloudwatch/export-history/{log_group_name.replace('/', '-')}"
                        
                      # Get the index parameter
                      index_param_name = f"{base_param_name}-index"
                      try:
                          response = ssm_client.get_parameter(Name=index_param_name)
                          index_data = json.loads(response['Parameter']['Value'])
                          total_params = index_data['totalParams']
                      except ssm_client.exceptions.ParameterNotFound:
                          logger.info(f"No task history found for {log_group_name}")
                          return []
                        
                      # Collect history from all parameters
                      complete_history = []
                      for i in range(total_params):
                          param_name = f"{base_param_name}-{i}"
                          try:
                              response = ssm_client.get_parameter(Name=param_name)
                              history_chunk = json.loads(response['Parameter']['Value'])
                              complete_history.extend(history_chunk)
                          except ssm_client.exceptions.ParameterNotFound:
                              logger.warning(f"Parameter {param_name} not found, but was expected")
                              continue
                        
                      # Sort by fromTime
                      complete_history.sort(key=lambda x: x['fromTime'])
                        
                      return complete_history
                  except Exception as e:
                      logger.error(f"Error retrieving task history: {str(e)}")
                      return []

              def verify_task_history(self, log_group_name, send_notification=True):
                  """Verify task history for overlaps across multiple parameters"""
                  try:
                      # Get complete history
                      history = self.get_task_history(log_group_name)
                        
                      if not history:
                          logger.info(f"No task history found for {log_group_name}")
                          return True
                       
                      # Check for overlaps
                      overlaps = []
                      for i in range(len(history) - 1):
                          current_task = history[i]
                          next_task = history[i+1]
                            
                          if current_task['toTime'] > next_task['fromTime']:
                              overlaps.append({
                                  'task1': current_task['taskId'],
                                  'task2': next_task['taskId'],
                                  'task1_range': f"{datetime.fromtimestamp(current_task['fromTime']/1000).isoformat()} to {datetime.fromtimestamp(current_task['toTime']/1000).isoformat()}",
                                  'task2_range': f"{datetime.fromtimestamp(next_task['fromTime']/1000).isoformat()} to {datetime.fromtimestamp(next_task['toTime']/1000).isoformat()}",
                                  'overlap': f"{datetime.fromtimestamp(next_task['fromTime']/1000).isoformat()} to {datetime.fromtimestamp(current_task['toTime']/1000).isoformat()}",
                                  'overlap_duration_ms': current_task['toTime'] - next_task['fromTime']
                              })
                        
                      if overlaps:
                          logger.error(f"Found {len(overlaps)} overlaps in task history for {log_group_name}")
                            
                          # Format detailed message for logging and notification
                          detail_message = f"Task Overlaps Detected for Log Group: {log_group_name}\n\n"
                          
                          for i, overlap in enumerate(overlaps):
                              logger.error(f"Overlap between tasks {overlap['task1']} and {overlap['task2']}: {overlap['overlap']}")
                                
                              # Add details to notification message
                              detail_message += f"Overlap #{i+1}:\n"
                              detail_message += f"?? Task 1 ID: {overlap['task1']}\n"
                              detail_message += f"?? Task 1 Range: {overlap['task1_range']}\n"
                              detail_message += f"?? Task 2 ID: {overlap['task2']}\n"
                              detail_message += f"?? Task 2 Range: {overlap['task2_range']}\n"
                              detail_message += f"?? Overlap Period: {overlap['overlap']}\n"
                              detail_message += f"?? Overlap Duration: {overlap['overlap_duration_ms']/1000/60:.2f} minutes\n\n"
                            
                          # Add remediation suggestions
                          detail_message += "Remediation Steps:\n"
                          detail_message += "1. Check the CloudWatch Logs export task history\n"
                          detail_message += "2. Verify the SSM Parameter Store values for last export times\n"
                          detail_message += "3. Consider manually adjusting the last export time parameter\n"
                          detail_message += "4. Review the locking mechanism implementation\n"
                            
                          # Send SNS notification if enabled
                          if send_notification and self.sns_topic_arn:
                              self.send_sns_notification(
                                  f"ALERT: Log Export Task Overlaps Detected - {log_group_name}",
                                  detail_message
                              )
                            
                          return False
                      else:
                          logger.info(f"No overlaps found in task history for {log_group_name}")
                          return True
                  except Exception as e:
                      error_msg = f"Error verifying task history: {str(e)}"
                      logger.error(error_msg)
                        
                      # Send notification about the verification error
                      if send_notification and self.sns_topic_arn:
                          self.send_sns_notification(
                              f"ERROR: Log Export Task Verification Failed - {log_group_name}",
                              f"Failed to verify task history for log group {log_group_name}.\n\nError: {str(e)}\n\nPlease check the Lambda logs for more details."
                          )
                        
                      return False

              def cleanup_task_history(self, log_group_name, max_age_days=90):
                  """Clean up old task history to prevent parameter store bloat"""
                  try:
                      # Get complete history
                      history = self.get_task_history(log_group_name)
                        
                      if not history:
                          logger.info(f"No task history found for {log_group_name}")
                          return True
                        
                      # Calculate cutoff timestamp
                      cutoff_time = int((datetime.now() - timedelta(days=max_age_days)).timestamp() * 1000)
                        
                      # Filter out old entries
                      new_history = [entry for entry in history if entry['timestamp'] > cutoff_time]
                        
                      if len(new_history) == len(history):
                          logger.info(f"No old task history to clean up for {log_group_name}")
                          return True
                        
                      logger.info(f"Cleaning up {len(history) - len(new_history)} old task history entries for {log_group_name}")
                       
                      # Base parameter name for this log group
                      base_param_name = f"/cloudwatch/export-history/{log_group_name.replace('/', '-')}"
                        
                      # Get the index parameter
                      index_param_name = f"{base_param_name}-index"
                        
                      # Delete all existing parameters
                      try:
                          response = ssm_client.get_parameter(Name=index_param_name)
                          index_data = json.loads(response['Parameter']['Value'])
                          total_params = index_data['totalParams']
                            
                          for i in range(total_params):
                              param_name = f"{base_param_name}-{i}"
                              try:
                                  ssm_client.delete_parameter(Name=param_name)
                              except ssm_client.exceptions.ParameterNotFound:
                                  continue
                            
                          # Delete the index parameter
                          ssm_client.delete_parameter(Name=index_param_name)
                      except ssm_client.exceptions.ParameterNotFound:
                          pass
                        
                      # Re-create parameters with cleaned history
                      if new_history:
                          # Split history into chunks of appropriate size
                          chunks = []
                          current_chunk = []
                          current_size = 0
                            
                          for entry in new_history:
                              entry_json = json.dumps(entry)
                              entry_size = len(entry_json.encode('utf-8'))
                                
                              if current_size + entry_size > 3500:  # 3.5KB limit to be safe
                                  chunks.append(current_chunk)
                                  current_chunk = [entry]
                                  current_size = entry_size
                              else:
                                  current_chunk.append(entry)
                                  current_size += entry_size
                            
                          if current_chunk:
                              chunks.append(current_chunk)
                            
                          # Create new parameters
                          for i, chunk in enumerate(chunks):
                              param_name = f"{base_param_name}-{i}"
                              ssm_client.put_parameter(
                                  Name=param_name,
                                  Value=json.dumps(chunk),
                                  Type='String'
                              )
                            
                          # Create new index parameter
                          ssm_client.put_parameter(
                              Name=index_param_name,
                              Value=json.dumps({'currentIndex': len(chunks) - 1, 'totalParams': len(chunks)}),
                              Type='String'
                          )
                            
                          logger.info(f"Re-created {len(chunks)} parameters for cleaned task history")
                        
                      return True
                  except Exception as e:
                      logger.error(f"Error cleaning up task history: {str(e)}")
                      return False


              def wait_for_export_task(self, task_id, log_group_name, current_chunk, total_chunks):
                  """Wait for export task completion with enhanced progress tracking"""
                  start_time = time.time()
                  last_log_time = start_time
                  last_status = None
                  
                  while True:
                      try:
                          if (time.time() - start_time) > (self.max_wait_time - 30):
                              # Modified timeout handling - Mark as COMPLETED since export will continue in background
                              logger.warning(f"Lambda timeout approaching for task {task_id}. Marking as successful since export will continue in background.")
                              self.update_export_status(log_group_name, task_id, 'COMPLETED', current_chunk, total_chunks)
                              
                              # Get task details for recording
                              response = self.check_export_task_status(task_id)
                              task = response['exportTasks'][0]
                              from_time = task.get('from', 0)
                              to_time = task.get('to', 0)

                              # Record as COMPLETED instead of TIMEOUT
                              self.record_task_execution(log_group_name, task_id, from_time, to_time, 'COMPLETED')
                              return True

                          response = self.check_export_task_status(task_id)
                          task = response['exportTasks'][0]
                          status = task['status']['code']
                          
                          # Get the actual from_time and to_time from the task
                          from_time = task.get('from', 0)
                          to_time = task.get('to', 0)
                          
                          # Only update status if it changed or 5 minutes passed
                          current_time = time.time()
                          if status != last_status or (current_time - last_log_time) >= 300:
                              self.update_export_status(log_group_name, task_id, status, current_chunk, total_chunks)
                              last_status = status
                              last_log_time = current_time
                          
                          if status == 'COMPLETED':
                              logger.info(f"Export task {task_id} completed successfully")
                              self.record_task_execution(log_group_name, task_id, from_time, to_time, 'COMPLETED')
                              return True
                          elif status in ['FAILED', 'CANCELLED']:
                              error_message = task['status'].get('message', 'No error message provided')
                              logger.error(f"Export task {task_id} failed: {error_message}")
                              self.record_task_execution(log_group_name, task_id, from_time, to_time, 'FAILED')
                              return False
                          
                          time.sleep(60)  # Check every minute
                          
                      except Exception as e:
                          logger.error(f"Error checking task status: {str(e)}")
                          time.sleep(60)  # Wait before retrying


              def check_export_task_status(self, task_id):
                  """Check export task status with retry logic"""
                  def check_status():
                      return logs_client.describe_export_tasks(taskId=task_id)
                  
                  try:
                      return self.retry_with_backoff(check_status)
                  except Exception as e:
                      logger.error(f"Error checking export task status: {str(e)}")
                      raise

              def check_existing_export_tasks(self):
                  """Check for any existing export tasks"""
                  try:
                      response = logs_client.describe_export_tasks()
                      active_tasks = [task for task in response.get('exportTasks', []) 
                                    if task['status']['code'] == 'PENDING' or 
                                      task['status']['code'] == 'RUNNING']
                      return len(active_tasks) > 0
                  except Exception as e:
                      logger.error(f"Error checking existing export tasks: {str(e)}")
                      return False
            
              def get_log_group_retention(self, log_group_name):
                  """Get the retention period of a log group in days"""
                  try:
                      response = logs_client.describe_log_groups(
                          logGroupNamePrefix=log_group_name,
                          limit=1
                      )
                      retention_in_days = response['logGroups'][0].get('retentionInDays', None)
                      return retention_in_days
                  except Exception as e:
                      logger.error(f"Error getting retention period for log group {log_group_name}: {str(e)}")
                      return None

              def export_log_group(self, log_group_name):
                  """Export a single log group with one chunk per Lambda invocation"""
                  try:
                      # Reset task start time for new export
                      self.current_task_start_time = time.time()

                      # Check for existing export tasks
                      if self.check_existing_export_tasks():
                          message = f"Skipping export for {log_group_name} - another export task is already running"
                          logger.warning(message)
                          self.send_sns_notification(
                              f"Log Export Skipped - {log_group_name}",
                              message
                          )
                          return False

                      # Get the progress parameter to determine which chunk to process
                      progress_param = self.get_ssm_parameter_name(log_group_name, 'export-progress')
                      
                      # Log the current parameter value
                      try:
                          current_param = ssm_client.get_parameter(Name=progress_param)
                          logger.info(f"Current progress parameter value: {current_param['Parameter']['Value']}")
                      except Exception as e:
                          logger.info(f"No existing progress parameter found: {str(e)}")

                      try:
                          response = ssm_client.get_parameter(Name=progress_param)
                          progress_data = json.loads(response['Parameter']['Value'])
                          current_chunk = progress_data.get('currentChunk', 0) + 1
                          total_chunks = progress_data.get('totalChunks')
                          last_export_time = progress_data.get('lastExportTime')
                          
                          # Log the parsed values
                          logger.info(f"Retrieved progress data - current_chunk: {current_chunk}, total_chunks: {total_chunks}, last_export_time: {last_export_time}")
                          
                      except ClientError as e:
                          if e.response['Error']['Code'] == 'ParameterNotFound':
                              # Initialize first chunk
                              current_chunk = 1
                              last_export_time = self.get_initial_export_time(log_group_name)
                              total_chunks = self.calculate_total_chunks(log_group_name, last_export_time)
                              logger.info(f"Initialized new progress - current_chunk: {current_chunk}, total_chunks: {total_chunks}, last_export_time: {last_export_time}")
                          else:
                              raise

                      # Always recalculate total_chunks based on current time
                      current_time = int(time.time() * 1000)
                      chunk_size = self.chunk_days * 24 * 60 * 60 * 1000
                      new_total_chunks = max(1, ((current_time - last_export_time) // chunk_size) + 1)
                      
                      logger.info(f"Chunk calculation - current_time: {current_time}, last_export_time: {last_export_time}, chunk_size: {chunk_size}")
                      logger.info(f"Chunks comparison - current total: {total_chunks}, calculated new total: {new_total_chunks}")

                      # Always update total_chunks to ensure we catch up to current time
                      if new_total_chunks != total_chunks:
                          total_chunks = new_total_chunks
                          logger.info(f"Updating total chunks to {total_chunks}")
                          
                          # Update the progress data with new total_chunks
                          progress_data = {
                              'currentChunk': current_chunk - 1,  # Subtract 1 because we added 1 earlier
                              'totalChunks': total_chunks,
                              'lastExportTime': last_export_time,
                              'status': 'IN_PROGRESS',
                              'lastUpdated': int(time.time() * 1000)
                          }
                          
                          try:
                              ssm_client.put_parameter(
                                  Name=progress_param,
                                  Value=json.dumps(progress_data),
                                  Type='String',
                                  Overwrite=True
                              )
                              logger.info(f"Successfully updated progress parameter with new total chunks: {json.dumps(progress_data)}")
                          except Exception as e:
                              logger.error(f"Failed to update progress parameter: {str(e)}")
                              raise

                      if current_chunk > total_chunks:
                          # Double check if we've really caught up to current time
                          if last_export_time < current_time:
                              logger.info(f"Despite chunk completion, there's still data to export. Resetting chunks.")
                              current_chunk = 1
                              total_chunks = new_total_chunks
                          else:
                              logger.info(f"All chunks completed for {log_group_name}")
                              return True

                      # Calculate time range for this chunk
                      from_time = last_export_time
                      to_time = min(from_time + chunk_size, current_time)

                      # Log the time range being processed
                      logger.info(f"Processing time range - from: {datetime.fromtimestamp(from_time/1000)} to {datetime.fromtimestamp(to_time/1000)}")

                      # Don't process if we've caught up to current time
                      if from_time >= to_time:
                          logger.info(f"No new data to export for {log_group_name}")
                          return True

                      logger.info(f"Processing chunk {current_chunk}/{total_chunks} for {log_group_name}")

                      # Create and monitor the export task
                      task_response = logs_client.create_export_task(
                          logGroupName=log_group_name,
                          fromTime=from_time,
                          to=to_time,
                          destination=self.destination_bucket,
                          destinationPrefix=f"{log_group_name.strip('/')}/{datetime.fromtimestamp(from_time/1000).strftime('%Y/%m/%d')}"
                      )
                      
                      task_id = task_response['taskId']
                      
                      # Wait for task completion with timeout handling
                      if self.wait_for_export_task(task_id, log_group_name, current_chunk, total_chunks):
                          # Update progress in parameter store
                          progress_data = {
                              'currentChunk': current_chunk,
                              'totalChunks': total_chunks,
                              'lastExportTime': to_time,
                              'status': 'IN_PROGRESS' if current_chunk < total_chunks else 'COMPLETED',
                              'lastUpdated': int(time.time() * 1000)
                          }
                          
                          try:
                              # Update the progress parameter
                              progress_param = self.get_ssm_parameter_name(log_group_name, 'export-progress')
                              ssm_client.put_parameter(
                                  Name=progress_param,
                                  Value=json.dumps(progress_data),
                                  Type='String',
                                  Overwrite=True
                              )
                              logger.info(f"Successfully updated progress after chunk completion: {json.dumps(progress_data)}")

                              # Update the last-export parameter
                              last_export_param = self.get_ssm_parameter_name(log_group_name, 'last-export')
                              ssm_client.put_parameter(
                                  Name=last_export_param,
                                  Value=str(to_time),  # Store the to_time as the last export time
                                  Type='String',
                                  Overwrite=True
                              )
                              logger.info(f"Successfully updated last-export parameter to: {datetime.fromtimestamp(to_time/1000).isoformat()}")

                          except Exception as e:
                              logger.error(f"Failed to update parameters after chunk completion: {str(e)}")
                              raise

                          # Send notification about chunk completion
                          message = (
                              f"Chunk {current_chunk}/{total_chunks} completed for {log_group_name}\n"
                              f"Time range: {datetime.fromtimestamp(from_time/1000)} to "
                              f"{datetime.fromtimestamp(to_time/1000)}"
                          )
                          self.send_sns_notification(
                              f"Log Export Progress - {log_group_name}",
                              message
                          )
                          
                          return True

                      return False

                  except Exception as e:
                      error_msg = f"Log export failed for {log_group_name}: {str(e)}"
                      logger.error(error_msg)
                      self.send_sns_notification(
                          f"Log Export Failed - {log_group_name}",
                          error_msg
                      )
                      return False


              def get_initial_export_time(self, log_group_name):
                  """Get the initial export time based on log group creation or retention period"""
                  try:
                      # Try to get last export time first
                      last_export_param = self.get_ssm_parameter_name(log_group_name, 'last-export')
                      response = ssm_client.get_parameter(Name=last_export_param)
                      return int(response['Parameter']['Value'])
                  except ClientError as e:
                      if e.response['Error']['Code'] == 'ParameterNotFound':
                          # Get log group details
                          response = logs_client.describe_log_groups(
                              logGroupNamePrefix=log_group_name,
                              limit=1
                          )
                          creation_time = response['logGroups'][0]['creationTime']
                          retention_days = response['logGroups'][0].get('retentionInDays')
                          
                          current_time = int(time.time() * 1000)
                          if retention_days:
                              # Use the later of creation time or retention-limited time
                              retention_limited_time = current_time - (retention_days * 24 * 60 * 60 * 1000)
                              return max(creation_time, retention_limited_time)
                          return creation_time
                      raise

              def calculate_total_chunks(self, log_group_name, start_time):
                  """Calculate total number of chunks needed"""
                  current_time = int(time.time() * 1000)
                  chunk_size = self.chunk_days * 24 * 60 * 60 * 1000
                  return max(1, ((current_time - start_time) // chunk_size) + 1)

              def export_all_tagged_logs(self):
                  """Main function to export all tagged log groups"""
                  tagged_groups = self.get_tagged_log_groups()
                  
                  if not tagged_groups:
                      logger.info(f"No log groups found with tag {self.export_tag_key}={self.export_tag_value}")
                      return
                  
                  success_count = 0
                  failure_count = 0
                  
                  # Use ThreadPoolExecutor for parallel processing
                  with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                      # Submit all export tasks
                      future_to_loggroup = {
                          executor.submit(self.export_log_group, log_group_name): log_group_name 
                          for log_group_name in tagged_groups
                      }
                      
                      # Process completed tasks
                      for future in concurrent.futures.as_completed(future_to_loggroup):
                          log_group_name = future_to_loggroup[future]
                          try:
                              if future.result():
                                  success_count += 1
                              else:
                                  failure_count += 1
                          except Exception as e:
                              logger.error(f"Export failed for {log_group_name}: {str(e)}")
                              failure_count += 1
                  
                  # Send summary notification
                  summary_message = (
                      f"Log Export Summary:\n"
                      f"Total log groups processed: {len(tagged_groups)}\n"
                      f"Successful exports: {success_count}\n"
                      f"Failed exports: {failure_count}"
                  )
                  self.send_sns_notification(
                      "Log Export Summary",
                      summary_message
                  )

          def lambda_handler(event, context):
              """AWS Lambda handler with simplified response"""
              try:
                  destination_bucket = event['destination_bucket']
                  sns_topic_arn = event['sns_topic_arn']
                  export_tag_key = event.get('export_tag_key', 'ExportToS3')
                  export_tag_value = event.get('export_tag_value', 'true')
                  chunk_days = event.get('chunk_days', 1)

                  # Check if this is a cleanup run
                  cleanup_only = event.get('cleanup_only', False)
                  max_age_days = event.get('max_age_days', 90)

                  # Check if this is a verification-only run
                  verification_only = event.get('verification_only', False)

                  exporter = LogExporter(
                      destination_bucket,
                      sns_topic_arn,
                      export_tag_key,
                      export_tag_value,
                      chunk_days,
                      context
                  )
                  
                  tagged_groups = exporter.get_tagged_log_groups()
                  
                  if not tagged_groups:
                      logger.info(f"No log groups found with tag {export_tag_key}={export_tag_value}")
                      return {
                          'statusCode': 200,
                          'body': 'No tagged log groups found'
                      }

                  results = {}
                  for log_group_name in tagged_groups:
                      if cleanup_only:
                          results[log_group_name] = exporter.cleanup_task_history(log_group_name, max_age_days)
                      elif verification_only:
                          results[log_group_name] = exporter.verify_task_history(log_group_name, send_notification=True)
                      else:
                          results[log_group_name] = exporter.export_log_group(log_group_name)

                  return {
                      'statusCode': 200,
                      'body': json.dumps({'results': results})
                  }

              except Exception as e:
                  error_msg = f"Error in lambda_handler: {str(e)}"
                  logger.error(error_msg)
                  return {
                      'statusCode': 500,
                      'body': f"Error: {str(e)}"
                  }


  LogExporterLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/${LogExporterFunction}'

  ExporterScheduleRule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub '${AWS::StackName}-schedule'
      Description: 'Schedule for CloudWatch Logs export to S3'
      ScheduleExpression: !Ref ScheduleExpression
      State: ENABLED
      Targets:
        - Arn: !GetAtt LogExporterFunction.Arn
          Id: 'LogExporterSchedule'
          Input: !Sub |
            {
              "destination_bucket": "${ExportBucket}",
              "sns_topic_arn": "${LogExporterSNSTopic}",
              "export_tag_key": "${ExportTagKey}",
              "export_tag_value": "${ExportTagValue}",
              "chunk_days": ${ChunkDays}
            }

  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref LogExporterFunction
      Action: 'lambda:InvokeFunction'
      Principal: 'events.amazonaws.com'
      SourceArn: !GetAtt ExporterScheduleRule.Arn

Outputs:
  ExportBucketName:
    Description: 'Name of the S3 bucket storing exported logs'
    Value: !Ref ExportBucket

  SNSTopicARN:
    Description: 'ARN of the SNS topic for notifications'
    Value: !Ref LogExporterSNSTopic

  LambdaFunctionARN:
    Description: 'ARN of the Lambda function'
    Value: !GetAtt LogExporterFunction.Arn

  ScheduleRuleARN:
    Description: 'ARN of the EventBridge schedule rule'
    Value: !GetAtt ExporterScheduleRule.Arn
