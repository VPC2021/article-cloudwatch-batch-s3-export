AWSTemplateFormatVersion: '2010-09-09'
Description: 'CloudFormation template for CloudWatch Logs to S3 export solution'

Parameters:
  NotificationEmail:
    Type: String
    Description: Email address to receive notifications
  
  ChunkDays:
    Type: Number
    Description: Number of days per export chunk
    MinValue: 1
    MaxValue: 30

  BucketName:
    Type: String
    Description: Name of the S3 bucket to store exported logs

  GlacierTransitionDays:
    Type: Number
    Description: Number of days after which objects transition to Glacier

  ExportTagKey:
    Type: String
    Description: Tag key to identify log groups for export
    
  ExportTagValue:
    Type: String
    Description: Tag value to identify log groups for export

  ScheduleExpression:
    Type: String
    Description: Schedule expression for the export task

Resources:
  ExportBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Retain
    UpdateReplacePolicy: Retain
    Properties:
      BucketName: !Ref BucketName
      VersioningConfiguration:
        Status: Enabled
      LifecycleConfiguration:
        Rules:
          - Id: GlacierTransition
            Status: Enabled
            ExpirationInDays: !Ref ObjectDeletionDays
            NoncurrentVersionExpirationInDays: !Ref ObjectDeletionDays
            Transitions:
              - TransitionInDays: !Ref GlacierTransitionDays
                StorageClass: GLACIER
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256

  ExportBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref ExportBucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: AllowCloudWatchLogsExport
            Effect: Allow
            Principal:
              Service: logs.amazonaws.com
            Action:
              - s3:GetBucketAcl
            Resource: !GetAtt ExportBucket.Arn
            Condition:
              StringEquals:
                aws:SourceAccount: !Ref AWS::AccountId
              ArnLike:
                aws:SourceArn: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:*'
          - Sid: AllowCloudWatchLogsPutObject
            Effect: Allow
            Principal:
              Service: logs.amazonaws.com
            Action:
              - s3:PutObject
            Resource: !Sub '${ExportBucket.Arn}/*'
            Condition:
              StringEquals:
                s3:x-amz-acl: bucket-owner-full-control
                aws:SourceAccount: !Ref AWS::AccountId
              ArnLike:
                aws:SourceArn: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:log-group:*'

  LogExporterSNSTopic:
    Type: AWS::SNS::Topic
    Properties:
      TopicName: !Sub '${AWS::StackName}-notifications'
      Subscription:
        - Protocol: email
          Endpoint: !Ref NotificationEmail

  LogExporterRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: LogExporterPermissions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateExportTask
                  - logs:DescribeExportTasks
                  - logs:DescribeLogGroups
                  - logs:ListTagsLogGroup
                Resource: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*'
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                Resource: !Sub '${ExportBucket.Arn}/*'
              - Effect: Allow
                Action:
                  - sns:Publish
                Resource: !Ref LogExporterSNSTopic
              - Effect: Allow
                Action:
                  - ssm:PutParameter
                  - ssm:GetParameter
                  - ssm:DeleteParameter
                Resource: '*'

  LogExporterFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${AWS::StackName}-exporter'
      Runtime: python3.9
      Handler: index.lambda_handler
      Timeout: 900
      MemorySize: 2048
      Role: !GetAtt LogExporterRole.Arn
      Environment:
        Variables:
          DESTINATION_BUCKET: !Ref ExportBucket
          SNS_TOPIC_ARN: !Ref LogExporterSNSTopic
          CHUNK_DAYS: !Ref ChunkDays
          MAX_CONCURRENT_EXPORTS: '1'
          ADAPTIVE_CHUNK_SIZE: 'true'
      Code:
        ZipFile: |
          import boto3
          import time
          from datetime import datetime, timezone, timedelta
          import logging
          import json
          from botocore.exceptions import ClientError
          from botocore.config import Config
          import os
          import random

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)
          
          # Initialize AWS clients with retry configuration
          retry_config = Config(
              retries = dict(
                  max_attempts = 10,
                  mode = 'adaptive'
              )
          )

          logs_client = boto3.client('logs', config=retry_config)
          ssm_client = boto3.client('ssm', config=retry_config)
          sns_client = boto3.client('sns', config=retry_config)

          def exponential_backoff(attempt, max_delay=32):
              """Calculate exponential backoff with jitter"""
              delay = min(max_delay, pow(2, attempt))
              jitter = random.uniform(0, 0.1 * delay)
              return delay + jitter

          class LogExporter:
              def __init__(self, destination_bucket, sns_topic_arn, export_tag_key, export_tag_value, chunk_days=7, lambda_context=None):
                  self.destination_bucket = destination_bucket
                  self.sns_topic_arn = sns_topic_arn
                  self.export_tag_key = export_tag_key
                  self.export_tag_value = export_tag_value
                  self.chunk_days = chunk_days
                  self.max_wait_time = 43200  # 12 hours in seconds
                  self.max_retries = 10
                  self.current_task_start_time = None
                  self.lambda_context = lambda_context

              def format_bytes(self, size):
                  """Convert bytes to human readable format"""
                  for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
                      if size < 1024.0:
                          return f"{size:.2f} {unit}"
                      size /= 1024.0

              def calculate_time_remaining(self, progress_percentage, elapsed_time):
                  """Estimate remaining time based on current progress"""
                  if progress_percentage <= 0:
                      return "Calculating..."
                  
                  elapsed_seconds = time.time() - self.current_task_start_time
                  total_estimated_seconds = (elapsed_seconds * 100) / progress_percentage
                  remaining_seconds = total_estimated_seconds - elapsed_seconds
                  
                  if remaining_seconds < 60:
                      return f"{int(remaining_seconds)} seconds"
                  elif remaining_seconds < 3600:
                      return f"{int(remaining_seconds/60)} minutes"
                  else:
                      return f"{remaining_seconds/3600:.1f} hours"

              def exponential_backoff(self, attempt, max_delay=32):
                  """Calculate exponential backoff with jitter"""
                  delay = min(max_delay, pow(2, attempt))
                  jitter = random.uniform(0, 0.1 * delay)
                  return delay + jitter

              def retry_with_backoff(self, func, *args, **kwargs):
                  """Generic retry function with exponential backoff"""
                  for attempt in range(self.max_retries):
                      try:
                          return func(*args, **kwargs)
                      except ClientError as e:
                          if e.response['Error']['Code'] in ['ThrottlingException', 'TooManyRequestsException']:
                              if attempt == self.max_retries - 1:
                                  raise  # Re-raise the exception if we've exhausted all retries
                              
                              delay = self.exponential_backoff(attempt)
                              logger.warning(f"Request throttled, retrying in {delay:.2f} seconds (attempt {attempt + 1}/{self.max_retries})")
                              time.sleep(delay)
                          else:
                              raise

              def calculate_optimal_chunk_size(self, log_group_name):
                  """Calculate optimal chunk size based on log volume"""
                  try:
                      response = logs_client.describe_log_groups(
                          logGroupNamePrefix=log_group_name,
                          limit=1
                      )
                      stored_bytes = response['logGroups'][0].get('storedBytes', 0)
                      
                      logger.info(f"Log group {log_group_name} size: {stored_bytes/1_000_000_000:.2f} GB")
                      
                      # Adjust chunk days based on volume
                      if stored_bytes > 10_000_000_000:  # 10 GB
                          optimal_days = 3  # 3 days for large log groups
                      elif stored_bytes > 5_000_000_000:  # 5 GB
                          optimal_days = 5  # 5 days for medium log groups
                      else:
                          optimal_days = 7  # 7 days for small log groups
                      
                      logger.info(f"Using {optimal_days} days chunk size for {log_group_name}")
                      return optimal_days
                          
                  except Exception as e:
                      logger.warning(f"Error calculating chunk size: {str(e)}, using default {self.chunk_days} days")
                      return self.chunk_days  # Return default chunk size

              def send_sns_notification(self, subject, message):
                  """Send an SNS notification with enhanced formatting"""
                  try:
                      if not self.sns_topic_arn:
                          logger.warning("SNS topic ARN not provided, skipping notification")
                          return False
                        
                      # Add timestamp and environment info to the message
                      formatted_message = f"""
              {message}

              ---
              Timestamp: {datetime.now().isoformat()}
              Environment: {os.environ.get('AWS_EXECUTION_ENV', 'Unknown')}
              Region: {os.environ.get('AWS_REGION', 'Unknown')}
              Lambda Function: {os.environ.get('AWS_LAMBDA_FUNCTION_NAME', 'Unknown')}
              """
                    
                      # Send the notification
                      sns_client.publish(
                          TopicArn=self.sns_topic_arn,
                          Subject=subject[:100],  # SNS subject has a 100 character limit
                          Message=formatted_message
                      )
                    
                      logger.info(f"Sent SNS notification: {subject}")
                      return True
                  except Exception as e:
                      logger.error(f"Failed to send SNS notification: {str(e)}")
                      return False
              
              def get_log_group_tags(self, log_group_name):
                  """Get tags for a log group with retry logic"""
                  def get_tags():
                      return logs_client.list_tags_log_group(logGroupName=log_group_name)
                  
                  try:
                      response = self.retry_with_backoff(get_tags)
                      return response.get('tags', {})
                  except Exception as e:
                      logger.error(f"Error getting tags for log group {log_group_name}: {str(e)}")
                      return {}

              def get_tagged_log_groups(self):
                  """Get all log groups with specified tag (with retry logic)"""
                  tagged_groups = []
                  next_token = None
                  
                  while True:
                      try:
                          if next_token:
                              response = self.retry_with_backoff(
                                  logs_client.describe_log_groups,
                                  nextToken=next_token
                              )
                          else:
                              response = self.retry_with_backoff(
                                  logs_client.describe_log_groups
                              )
                          
                          for log_group in response.get('logGroups', []):
                              log_group_name = log_group['logGroupName']
                              tags = self.get_log_group_tags(log_group_name)
                              
                              if tags.get(self.export_tag_key) == self.export_tag_value:
                                  logger.info(f"Found tagged log group: {log_group_name}")
                                  tagged_groups.append(log_group_name)
                          
                          next_token = response.get('nextToken')
                          if not next_token:
                              break
                              
                      except Exception as e:
                          logger.error(f"Error listing log groups: {str(e)}")
                          break
                  
                  return tagged_groups

              def get_ssm_parameter_name(self, log_group_name, param_type='last-export'):
                  """Generate SSM parameter name for different tracking purposes"""
                  sanitized_name = log_group_name.lstrip('/').replace('/', '-')
                  return f"/log-exporter/{sanitized_name}/{param_type}"

              def store_export_progress(self, log_group_name, progress_data):
                  """Store export progress in SSM Parameter Store"""
                  try:
                      parameter_name = self.get_ssm_parameter_name(log_group_name, 'progress')
                      ssm_client.put_parameter(
                          Name=parameter_name,
                          Value=json.dumps(progress_data),
                          Type='String',
                          Overwrite=True
                      )
                      logger.info(f"Stored progress data for {log_group_name}")
                  except Exception as e:
                      logger.error(f"Error storing progress data: {str(e)}")

              def get_export_progress(self, log_group_name):
                  """Retrieve export progress from SSM Parameter Store"""
                  try:
                      parameter_name = self.get_ssm_parameter_name(log_group_name, 'progress')
                      response = ssm_client.get_parameter(Name=parameter_name)
                      return json.loads(response['Parameter']['Value'])
                  except ClientError as e:
                      if e.response['Error']['Code'] == 'ParameterNotFound':
                          return None
                      raise
                  except Exception as e:
                      logger.error(f"Error retrieving progress data: {str(e)}")
                      return None

              def update_export_status(self, log_group_name, task_id, status, current_chunk, total_chunks):
                  """Update export status in SSM Parameter Store with enhanced progress tracking"""
                  try:
                      if self.current_task_start_time is None:
                          self.current_task_start_time = time.time()

                      elapsed_time = time.time() - self.current_task_start_time
                      percentage = (current_chunk / total_chunks) * 100
                      
                      progress_data = {
                          'taskId': task_id,
                          'status': status,
                          'currentChunk': current_chunk,
                          'totalChunks': total_chunks,
                          'lastUpdated': int(time.time() * 1000),
                          'percentage': percentage,
                          'elapsedTime': elapsed_time,
                          'estimatedTimeRemaining': self.calculate_time_remaining(percentage, elapsed_time)
                      }
                      
                      self.store_export_progress(log_group_name, progress_data)
                      
                      # Enhanced progress logging
                      progress_message = (
                          f"\nExport Progress for {log_group_name}:\n"
                          f"├─ Status: {status}\n"
                          f"├─ Progress: {percentage:.1f}% ({current_chunk}/{total_chunks} chunks)\n"
                          f"├─ Elapsed Time: {int(elapsed_time/60)} minutes {int(elapsed_time%60)} seconds\n"
                          f"└─ Estimated Time Remaining: {progress_data['estimatedTimeRemaining']}"
                      )
                      
                      logger.info(progress_message)
                      
                      # Send notification at 25% intervals or completion
                      if percentage % 25 < 1 or status == 'COMPLETED':
                          self.send_sns_notification(
                              f"Log Export Progress - {log_group_name}",
                              progress_message
                          )
                          
                  except Exception as e:
                      logger.error(f"Error updating export status: {str(e)}")

              def get_last_export_time(self, ssm_parameter_name):
                  """Get last export timestamp from SSM Parameter Store"""
                  try:
                      response = ssm_client.get_parameter(Name=ssm_parameter_name)
                      return int(response['Parameter']['Value'])
                  except ssm_client.exceptions.ParameterNotFound:
                      logger.info(f"SSM parameter {ssm_parameter_name} not found, creating with default value")
                      # Set a default timestamp (e.g., 24 hours ago)
                      default_timestamp = int((datetime.now() - timedelta(days=1)).timestamp() * 1000)
                      self.update_last_export_time(ssm_parameter_name, default_timestamp)
                      return default_timestamp
                  except Exception as e:
                      logger.error(f"Error retrieving SSM parameter: {str(e)}")
                      raise

              def check_and_acquire_lock(self, lock_parameter_name):
                  """Try to acquire a lock using SSM Parameter Store"""
                  try:
                      # Try to create a lock parameter with current timestamp
                      current_time = int(time.time())
                      expiry_time = current_time + self.max_wait_time  # Lock expires after max_wait_time

                      # Generate a unique ID if lambda_context is not available
                      request_id = self.lambda_context.aws_request_id if self.lambda_context else f"local-{uuid.uuid4()}"
                    
                      try:
                          # Try to create the parameter if it doesn't exist
                          ssm_client.put_parameter(
                              Name=lock_parameter_name,
                              Value=f"{expiry_time}|{self.lambda_context.aws_request_id}",
                              Type='String',
                              Overwrite=False  # Don't overwrite if it exists
                          )
                          logger.info(f"Acquired lock {lock_parameter_name}")
                          return True
                      except ssm_client.exceptions.ParameterAlreadyExists:
                          # Parameter exists, check if lock has expired
                          response = ssm_client.get_parameter(Name=lock_parameter_name)
                          lock_value = response['Parameter']['Value']
                          parts = lock_value.split('|')
                        
                          if len(parts) == 2:
                              lock_expiry, lock_owner = int(parts[0]), parts[1]
                             
                              if current_time > lock_expiry:
                                  # Lock has expired, take it over
                                  logger.info(f"Taking over expired lock {lock_parameter_name} from {lock_owner}")
                                  ssm_client.put_parameter(
                                      Name=lock_parameter_name,
                                      Value=f"{expiry_time}|{self.lambda_context.aws_request_id}",
                                      Type='String',
                                      Overwrite=True
                                  )
                                  return True
                              else:
                                  # Lock is still valid
                                  logger.info(f"Lock {lock_parameter_name} is held by {lock_owner}, cannot proceed")
                                  return False
                          else:
                              # Invalid lock format, overwrite it
                              logger.warning(f"Invalid lock format in {lock_parameter_name}, overwriting")
                              ssm_client.put_parameter(
                                  Name=lock_parameter_name,
                                  Value=f"{expiry_time}|{self.lambda_context.aws_request_id}",
                                  Type='String',
                                  Overwrite=True
                              )
                              return True
                  except Exception as e:
                      logger.error(f"Error acquiring lock: {str(e)}")
                      return False

              def release_lock(self, lock_parameter_name):
                  """Release the acquired lock"""
                  try:
                      response = ssm_client.get_parameter(Name=lock_parameter_name)
                      lock_value = response['Parameter']['Value']
                      parts = lock_value.split('|')

                      request_id = self.lambda_context.aws_request_id if self.lambda_context else None
                      
                      if len(parts) == 2 and (parts[1] == request_id or request_id is None):
                          # Only delete if we own the lock
                          ssm_client.delete_parameter(Name=lock_parameter_name)
                          logger.info(f"Released lock {lock_parameter_name}")
                          return True
                      else:
                          logger.warning(f"Cannot release lock {lock_parameter_name} as it's owned by another process")
                          return False
                  except Exception as e:
                      logger.error(f"Error releasing lock: {str(e)}")
                      return False

              def update_last_export_time(self, ssm_parameter_name, timestamp):
                  """Update last export timestamp in SSM Parameter Store"""
                  try:
                      ssm_client.put_parameter(
                          Name=ssm_parameter_name,
                          Value=str(timestamp),
                          Type='String',
                          Overwrite=True
                      )
                      logger.info(f"Updated SSM parameter {ssm_parameter_name} with timestamp {timestamp}")
                  except Exception as e:
                      logger.error(f"Failed to update SSM parameter: {str(e)}")
                      raise

              def record_task_execution(self, log_group_name, task_id, from_time, to_time, status):
                  """Record task execution details in SSM Parameter Store"""
                  try:
                      history_param_name = f"/cloudwatch/export-history/{log_group_name.replace('/', '-')}"
                      
                      # Get existing history
                      try:
                          response = ssm_client.get_parameter(Name=history_param_name)
                          history = json.loads(response['Parameter']['Value'])
                      except ssm_client.exceptions.ParameterNotFound:
                          history = []
                    
                      # Add new entry
                      history.append({
                          'taskId': task_id,
                          'fromTime': from_time,
                          'toTime': to_time,
                          'fromTimeFormatted': datetime.fromtimestamp(from_time/1000).isoformat(),
                          'toTimeFormatted': datetime.fromtimestamp(to_time/1000).isoformat(),
                          'status': status,
                          'timestamp': int(time.time() * 1000)
                      })
                    
                      # Keep only the last 100 entries
                      if len(history) > 100:
                          history = history[-100:]
                      
                      # Update parameter
                      ssm_client.put_parameter(
                          Name=history_param_name,
                          Value=json.dumps(history),
                          Type='String',
                          Overwrite=True
                      )
                    
                      logger.info(f"Recorded task execution for {log_group_name}, task ID: {task_id}")
                  except Exception as e:
                      logger.error(f"Error recording task execution: {str(e)}")

              def verify_task_history(self, log_group_name):
                  """Verify task history for overlaps"""
                  try:
                      history_param_name = f"/cloudwatch/export-history/{log_group_name.replace('/', '-')}"
                    
                      try:
                          response = ssm_client.get_parameter(Name=history_param_name)
                          history = json.loads(response['Parameter']['Value'])
                      except ssm_client.exceptions.ParameterNotFound:
                          logger.info(f"No task history found for {log_group_name}")
                          return True
                    
                      # Sort by fromTime
                      history.sort(key=lambda x: x['fromTime'])
                    
                      # Check for overlaps
                      overlaps = []
                      for i in range(len(history) - 1):
                          current_task = history[i]
                          next_task = history[i+1]
                        
                          if current_task['toTime'] > next_task['fromTime']:
                              overlaps.append({
                                  'task1': current_task['taskId'],
                                  'task2': next_task['taskId'],
                                  'task1_range': f"{datetime.fromtimestamp(current_task['fromTime']/1000).isoformat()} to {datetime.fromtimestamp(current_task['toTime']/1000).isoformat()}",
                                  'task2_range': f"{datetime.fromtimestamp(next_task['fromTime']/1000).isoformat()} to {datetime.fromtimestamp(next_task['toTime']/1000).isoformat()}",
                                  'overlap': f"{datetime.fromtimestamp(next_task['fromTime']/1000).isoformat()} to {datetime.fromtimestamp(current_task['toTime']/1000).isoformat()}",
                                  'overlap_duration_ms': current_task['toTime'] - next_task['fromTime']
                              })
                    
                      if overlaps:
                          logger.error(f"Found {len(overlaps)} overlaps in task history for {log_group_name}")
            
                          # Format detailed message for logging and notification
                          detail_message = f"Task Overlaps Detected for Log Group: {log_group_name}\n\n"
                        
                          for i, overlap in enumerate(overlaps):
                              logger.error(f"Overlap between tasks {overlap['task1']} and {overlap['task2']}: {overlap['overlap']}")
                            
                              # Add details to notification message
                              detail_message += f"Overlap #{i+1}:\n"
                              detail_message += f"├─ Task 1 ID: {overlap['task1']}\n"
                              detail_message += f"├─ Task 1 Range: {overlap['task1_range']}\n"
                              detail_message += f"├─ Task 2 ID: {overlap['task2']}\n"
                              detail_message += f"├─ Task 2 Range: {overlap['task2_range']}\n"
                              detail_message += f"├─ Overlap Period: {overlap['overlap']}\n"
                              detail_message += f"└─ Overlap Duration: {overlap['overlap_duration_ms']/1000/60:.2f} minutes\n\n"
                        
                          # Add remediation suggestions
                          detail_message += "Remediation Steps:\n"
                          detail_message += "1. Check the CloudWatch Logs export task history\n"
                          detail_message += "2. Verify the SSM Parameter Store values for last export times\n"
                          detail_message += "3. Consider manually adjusting the last export time parameter\n"
                          detail_message += "4. Review the locking mechanism implementation\n"
                        
                          # Send SNS notification if enabled
                          if send_notification and self.sns_topic_arn:
                              self.send_sns_notification(
                                  f"ALERT: Log Export Task Overlaps Detected - {log_group_name}",
                                  detail_message
                              )
                        
                          return False
                      else:
                          logger.info(f"No overlaps found in task history for {log_group_name}")
                          return True
                  except Exception as e:
                      error_msg = f"Error verifying task history: {str(e)}"
                      logger.error(error_msg)
                      
                      # Send notification about the verification error
                      if send_notification and self.sns_topic_arn:
                          self.send_sns_notification(
                              f"ERROR: Log Export Task Verification Failed - {log_group_name}",
                              f"Failed to verify task history for log group {log_group_name}.\n\nError: {str(e)}\n\nPlease check the Lambda logs for more details."
                          )
                    
                      return False

              def wait_for_export_task(self, task_id, log_group_name, current_chunk, total_chunks):
                  """Wait for export task completion with enhanced progress tracking"""
                  start_time = time.time()
                  last_log_time = start_time
                  last_status = None
                    
                  while True:
                      try:
                          if (time.time() - start_time) > self.max_wait_time:
                              self.update_export_status(log_group_name, task_id, 'TIMEOUT', current_chunk, total_chunks)
                              # Update task history with timeout status
                              self.record_task_execution(log_group_name, task_id, 0, 0, 'TIMEOUT')
                              raise TimeoutError(f"Export task {task_id} exceeded maximum wait time of 12 hours")

                          response = self.check_export_task_status(task_id)
                          task = response['exportTasks'][0]
                          status = task['status']['code']
                          
                          # Get the actual from_time and to_time from the task
                          from_time = task.get('from', 0)
                          to_time = task.get('to', 0)
                            
                          # Only update status if it changed or 5 minutes passed
                          current_time = time.time()
                          if status != last_status or (current_time - last_log_time) >= 300:
                              self.update_export_status(log_group_name, task_id, status, current_chunk, total_chunks)
                              last_status = status
                              last_log_time = current_time
                            
                          if status == 'COMPLETED':
                              logger.info(f"Export task {task_id} completed successfully")
                              # Update task history with completed status
                              self.record_task_execution(log_group_name, task_id, from_time, to_time, 'COMPLETED')
                              return True
                          elif status in ['FAILED', 'CANCELLED']:
                              error_message = task['status'].get('message', 'No error message provided')
                              logger.error(f"Export task {task_id} failed: {error_message}")
                              # Update task history with failed status
                              self.record_task_execution(log_group_name, task_id, from_time, to_time, 'FAILED')
                              return False
                            
                          time.sleep(60)  # Check every minute
                            
                      except Exception as e:
                          logger.error(f"Error checking task status: {str(e)}")
                          time.sleep(60)  # Wait before retrying

              def create_export_task(self, log_group_name, from_time, to_time):
                  """Create export task with retry logic and non-overlapping time ranges"""
                  # Lock parameter name for this log group
                  lock_parameter_name = f"/cloudwatch/export-lock/{log_group_name.replace('/', '-')}"
                  last_export_param_name = f"/cloudwatch/last-export/{log_group_name.replace('/', '-')}"
                
                  # Try to acquire the lock
                  if not self.check_and_acquire_lock(lock_parameter_name):
                      logger.info(f"Another export task is already running for {log_group_name}, skipping")
                      return None
                
                  try:
                      # Get the last export time
                      last_export_time = self.get_last_export_time(last_export_param_name)
                     
                      # Ensure from_time is after the last export time
                      if from_time <= last_export_time:
                          logger.info(f"Adjusting from_time to avoid overlap with previous export")
                          from_time = last_export_time + 1
                    
                      # Ensure to_time is after from_time
                      if to_time <= from_time:
                          logger.info(f"to_time {to_time} is not after from_time {from_time}, skipping export")
                          return None
                    
                      logger.info(f"Creating export task for {log_group_name} from {from_time} to {to_time}")
                     
                      def create_task():
                          return logs_client.create_export_task(
                              logGroupName=log_group_name,
                              fromTime=from_time,
                              to=to_time,
                              destination=self.destination_bucket,
                              destinationPrefix=f"{log_group_name.strip('/')}/{datetime.fromtimestamp(from_time/1000).strftime('%Y/%m/%d')}"
                          )
                    
                      response = self.retry_with_backoff(create_task)
                    
                      # Update the last export time to the to_time of this export
                      self.update_last_export_time(last_export_param_name, to_time)
                    
                      return response
                  except Exception as e:
                      logger.error(f"Error creating export task: {str(e)}")
                      raise
                  finally:
                      # Always release the lock when done
                      self.release_lock(lock_parameter_name)

              def check_export_task_status(self, task_id):
                  """Check export task status with retry logic"""
                  def check_status():
                      return logs_client.describe_export_tasks(taskId=task_id)
                  
                  try:
                      return self.retry_with_backoff(check_status)
                  except Exception as e:
                      logger.error(f"Error checking export task status: {str(e)}")
                      raise

              def check_existing_export_tasks(self):
                  """Check for any existing export tasks"""
                  try:
                      response = logs_client.describe_export_tasks()
                      active_tasks = [task for task in response.get('exportTasks', []) 
                                    if task['status']['code'] == 'PENDING' or 
                                      task['status']['code'] == 'RUNNING']
                      return len(active_tasks) > 0
                  except Exception as e:
                      logger.error(f"Error checking existing export tasks: {str(e)}")
                      return False
            
              def get_log_group_retention(self, log_group_name):
                  """Get the retention period of a log group in days"""
                  try:
                      response = logs_client.describe_log_groups(
                          logGroupNamePrefix=log_group_name,
                          limit=1
                      )
                      retention_in_days = response['logGroups'][0].get('retentionInDays', None)
                      return retention_in_days
                  except Exception as e:
                      logger.error(f"Error getting retention period for log group {log_group_name}: {str(e)}")
                      return None

              def export_log_group(self, log_group_name):
                  """Export a single log group with progress tracking and retention period handling"""
                  try:
                      # Reset task start time for new export
                      self.current_task_start_time = None

                      if self.check_existing_export_tasks():
                          message = f"Skipping export for {log_group_name} - another export task is already running"
                          logger.warning(message)
                          self.send_sns_notification(
                              f"Log Export Skipped - {log_group_name}",
                              message
                          )
                          return False

                      # Get retention period first
                      retention_days = self.get_log_group_retention(log_group_name)
                      current_time = int(time.time() * 1000)

                      # Adjust chunk size based on retention period
                      if retention_days and self.chunk_days > retention_days:
                          logger.info(f"Adjusting chunk size from {self.chunk_days} to {retention_days} days to match retention period")
                          chunk_size = retention_days * 24 * 60 * 60 * 1000
                      else:
                          chunk_size = self.chunk_days * 24 * 60 * 60 * 1000

                      # Get last export time
                      last_export_param = self.get_ssm_parameter_name(log_group_name, 'last-export')
                      try:
                          response = ssm_client.get_parameter(Name=last_export_param)
                          last_export_time = int(response['Parameter']['Value'])
                      except ClientError as e:
                          if e.response['Error']['Code'] == 'ParameterNotFound':
                              # Get log group creation time if no last export
                              response = logs_client.describe_log_groups(
                                  logGroupNamePrefix=log_group_name,
                                  limit=1
                              )
                              creation_time = response['logGroups'][0]['creationTime']
                
                              if retention_days:
                                  # Use the later of creation time or retention-limited time
                                  retention_limited_time = current_time - (retention_days * 24 * 60 * 60 * 1000)
                                  last_export_time = max(creation_time, retention_limited_time)
                              else:
                                  last_export_time = creation_time
                          else:
                              raise

                      # Ensure we don't exceed retention period if it exists
                      if retention_days:
                          earliest_allowed_time = current_time - (retention_days * 24 * 60 * 60 * 1000)
                          if last_export_time < earliest_allowed_time:
                              logger.info(f"Adjusting start time to match retention period of {retention_days} days")
                              last_export_time = earliest_allowed_time

                      chunk_size = self.chunk_days * 24 * 60 * 60 * 1000
                      total_chunks = ((current_time - last_export_time) // chunk_size) + 1

                      logger.info(f"Starting export for {log_group_name}")
                      logger.info(f"Time range: {datetime.fromtimestamp(last_export_time/1000)} to "
                              f"{datetime.fromtimestamp(current_time/1000)}")
                      logger.info(f"Retention period: {retention_days if retention_days else 'infinite'} days")
                      logger.info(f"Total chunks to process: {total_chunks}")

                      # Initialize progress
                      self.store_export_progress(log_group_name, {
                          'status': 'STARTING',
                          'currentChunk': 0,
                          'totalChunks': total_chunks,
                          'startTime': last_export_time,
                          'endTime': current_time,
                          'lastUpdated': int(time.time() * 1000),
                          'retentionDays': retention_days,
                          'chunkSizeDays': chunk_size/(24*60*60*1000)
                      })

                      start_time = last_export_time
                      chunks_processed = 0
                      
                      while start_time < current_time:
                          end_time = min(start_time + chunk_size, current_time)
                          chunks_processed += 1
                          
                          logger.info(f"Processing chunk {chunks_processed}/{total_chunks}")
                          logger.info(f"Time range: {datetime.fromtimestamp(start_time/1000)} to "
                                    f"{datetime.fromtimestamp(end_time/1000)}")

                          task_response = logs_client.create_export_task(
                              logGroupName=log_group_name,
                              fromTime=start_time,
                              to=end_time,
                              destination=self.destination_bucket,
                              destinationPrefix=f"{log_group_name.strip('/')}/{datetime.fromtimestamp(start_time/1000).strftime('%Y/%m/%d')}"
                          )
                          
                          task_id = task_response['taskId']
                          
                          if not self.wait_for_export_task(task_id, log_group_name, chunks_processed, total_chunks):
                              raise Exception(f"Failed to export chunk {chunks_processed}")

                          # Update last export time
                          ssm_client.put_parameter(
                              Name=last_export_param,
                              Value=str(end_time),
                              Type='String',
                              Overwrite=True
                          )
                          
                          start_time = end_time

                      # Final progress update
                      self.store_export_progress(log_group_name, {
                          'status': 'COMPLETED',
                          'currentChunk': total_chunks,
                          'totalChunks': total_chunks,
                          'startTime': last_export_time,
                          'endTime': current_time,
                          'lastUpdated': int(time.time() * 1000),
                          'percentage': 100,
                          'retentionDays': retention_days
                      })

                      # Add summary at completion
                      total_time = time.time() - self.current_task_start_time
                      summary_message = (
                          f"\nExport Summary for {log_group_name}:\n"
                          f"├─ Total Chunks Processed: {chunks_processed}\n"
                          f"├─ Time Range: {datetime.fromtimestamp(last_export_time/1000)} to "
                          f"{datetime.fromtimestamp(current_time/1000)}\n"
                          f"├─ Retention Period: {retention_days if retention_days else 'infinite'} days\n"
                          f"├─ Total Duration: {int(total_time/60)} minutes {int(total_time%60)} seconds\n"
                          f"└─ Average Time per Chunk: {(total_time/chunks_processed/60):.1f} minutes"
                      )
                      
                      logger.info(summary_message)
                      self.send_sns_notification(
                          f"Log Export Complete - {log_group_name}",
                          summary_message
                      )
                      
                      return True

                  except Exception as e:
                      error_msg = f"Log export failed for {log_group_name}: {str(e)}"
                      logger.error(error_msg)
                      self.send_sns_notification(
                          f"Log Export Failed - {log_group_name}",
                          error_msg
                      )
                      return False
                      
                      # Update progress with error status
                      self.store_export_progress(log_group_name, {
                          'status': 'FAILED',
                          'error': str(e),
                          'currentChunk': chunks_processed,
                          'totalChunks': total_chunks,
                          'lastUpdated': int(time.time() * 1000),
                          'retentionDays': retention_days
                      })
                      
                      self.send_sns_notification(
                          f"Log Export Failed - {log_group_name}",
                          error_msg
                      )
                      return False

              def get_export_summary(self, log_group_name):
                  """Get summary of export progress"""
                  progress_data = self.get_export_progress(log_group_name)
                  if not progress_data:
                      return "No export data available"
                      
                  status = progress_data['status']
                  percentage = progress_data.get('percentage', 0)
                  current_chunk = progress_data.get('currentChunk', 0)
                  total_chunks = progress_data.get('totalChunks', 0)
                  
                  return (f"Export Status: {status}\n"
                          f"Progress: {percentage:.2f}%\n"
                          f"Chunks Processed: {current_chunk}/{total_chunks}")

              def export_all_tagged_logs(self):
                  """Main function to export all tagged log groups"""
                  tagged_groups = self.get_tagged_log_groups()
                  
                  if not tagged_groups:
                      logger.info(f"No log groups found with tag {self.export_tag_key}={self.export_tag_value}")
                      return
                  
                  success_count = 0
                  failure_count = 0
                  
                  # Use ThreadPoolExecutor for parallel processing
                  with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                      # Submit all export tasks
                      future_to_loggroup = {
                          executor.submit(self.export_log_group, log_group_name): log_group_name 
                          for log_group_name in tagged_groups
                      }
                      
                      # Process completed tasks
                      for future in concurrent.futures.as_completed(future_to_loggroup):
                          log_group_name = future_to_loggroup[future]
                          try:
                              if future.result():
                                  success_count += 1
                              else:
                                  failure_count += 1
                          except Exception as e:
                              logger.error(f"Export failed for {log_group_name}: {str(e)}")
                              failure_count += 1
                  
                  # Send summary notification
                  summary_message = (
                      f"Log Export Summary:\n"
                      f"Total log groups processed: {len(tagged_groups)}\n"
                      f"Successful exports: {success_count}\n"
                      f"Failed exports: {failure_count}"
                  )
                  self.send_sns_notification(
                      "Log Export Summary",
                      summary_message
                  )

          def lambda_handler(event, context):
              """AWS Lambda handler with SSM-based progress tracking"""
              try:
                  destination_bucket = event['destination_bucket']
                  sns_topic_arn = event['sns_topic_arn']
                  export_tag_key = event.get('export_tag_key', 'ExportToS3')
                  export_tag_value = event.get('export_tag_value', 'true')
                  chunk_days = event.get('chunk_days', 7)

                  # Check if this is a verification-only run
                  verification_only = event.get('verification_only', False)

                  exporter = LogExporter(
                      destination_bucket,
                      sns_topic_arn,
                      export_tag_key,
                      export_tag_value,
                      chunk_days,
                      context
                  )
                  
                  tagged_groups = exporter.get_tagged_log_groups()
                  
                  if not tagged_groups:
                      logger.info(f"No log groups found with tag {export_tag_key}={export_tag_value}")
                      return {
                          'statusCode': 200,
                          'body': 'No tagged log groups found'
                      }
                  
                  # If this is a verification-only run, just check for overlaps
                  if verification_only:
                      verification_results = {}
                      overlap_count = 0
                    
                      for log_group_name in tagged_groups:
                          is_valid = exporter.verify_task_history(log_group_name, send_notification=True)
                          verification_results[log_group_name] = is_valid
                          if not is_valid:
                              overlap_count += 1
                    
                      # Send a summary notification if overlaps were found
                      if overlap_count > 0 and sns_topic_arn:
                          exporter.send_sns_notification(
                              f"Log Export Task Verification Summary - {overlap_count} Log Groups with Overlaps",
                              f"Detected overlaps in {overlap_count} out of {len(tagged_groups)} log groups.\n\n"
                              f"Please check individual notifications for details or review the Lambda logs."
                          )
                    
                      return {
                          'statusCode': 200,
                          'body': json.dumps({
                              'verification_results': verification_results,
                              'overlap_count': overlap_count
                          })
                      }

                  results = {}
                  verification_results = {}
                
                  # First verify all log groups for overlaps
                  for log_group_name in tagged_groups:
                      verification_results[log_group_name] = exporter.verify_task_history(log_group_name)
                
                  # Then process each log group
                  for log_group_name in tagged_groups:
                      success = exporter.export_log_group(log_group_name)
                      results[log_group_name] = {
                          'success': success,
                          'summary': exporter.get_export_summary(log_group_name)
                      }
                
                  # Verify again after processing
                  post_verification = {}
                  for log_group_name in tagged_groups:
                      post_verification[log_group_name] = exporter.verify_task_history(log_group_name)
                
                  return {
                      'statusCode': 200,
                      'body': json.dumps({
                          'results': results,
                          'preVerification': verification_results,
                          'postVerification': post_verification
                      })
                  }
              except Exception as e:
                  error_msg = f"Error in lambda_handler: {str(e)}"
                  logger.error(error_msg)
                
                  # Send notification about the error
                  try:
                      sns_topic_arn = event.get('sns_topic_arn')
                      if sns_topic_arn:
                          sns_client.publish(
                              TopicArn=sns_topic_arn,
                              Subject=f"ERROR: Log Export Lambda Failure",
                              Message=f"The Log Export Lambda function encountered an error:\n\n{error_msg}\n\nPlease check the Lambda logs for more details."
                          )
                  except Exception as sns_error:
                      logger.error(f"Failed to send error notification: {str(sns_error)}")
                
                  return {
                      'statusCode': 500,
                      'body': f"Error: {str(e)}"
                  }

  LogExporterLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws/lambda/${LogExporterFunction}'

  ExporterScheduleRule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub '${AWS::StackName}-schedule'
      Description: 'Schedule for CloudWatch Logs export to S3'
      ScheduleExpression: !Ref ScheduleExpression
      State: ENABLED
      Targets:
        - Arn: !GetAtt LogExporterFunction.Arn
          Id: 'LogExporterSchedule'
          Input: !Sub |
            {
              "destination_bucket": "${ExportBucket}",
              "sns_topic_arn": "${LogExporterSNSTopic}",
              "export_tag_key": "${ExportTagKey}",
              "export_tag_value": "${ExportTagValue}",
              "chunk_days": ${ChunkDays}
            }

  LambdaInvokePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref LogExporterFunction
      Action: 'lambda:InvokeFunction'
      Principal: 'events.amazonaws.com'
      SourceArn: !GetAtt ExporterScheduleRule.Arn

Outputs:
  ExportBucketName:
    Description: 'Name of the S3 bucket storing exported logs'
    Value: !Ref ExportBucket

  SNSTopicARN:
    Description: 'ARN of the SNS topic for notifications'
    Value: !Ref LogExporterSNSTopic

  LambdaFunctionARN:
    Description: 'ARN of the Lambda function'
    Value: !GetAtt LogExporterFunction.Arn

  ScheduleRuleARN:
    Description: 'ARN of the EventBridge schedule rule'
    Value: !GetAtt ExporterScheduleRule.Arn
